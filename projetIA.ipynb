{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Etude théorique de cas simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Influence de η"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas **η = 0**: La valeur des poids du neurone **ne changeront pas**. En effet η est le taux d’apprentissage, il permet de configurer la vitesse à laquelle les neurones doivent apprendre à chaque itération. S’il vaut 0, les neurones n’évolueront pas entre deux itérations. \n",
    "\n",
    "$w_{ij}=0e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=0$ -> Aucune évolutions des poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas **η=1**, la valeur des poids du neurone **seront celle de l’entrée courante**. Les neurones auront appris de l’itération et les poids auront évoluer. Comme η=1, ils auront compensé toute la distance qui sépare notre neurone à l’entrée courante.\n",
    "\n",
    "$w_{ij}=1e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=e^{\\frac{-||0||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=e^{0}(x_{i}-w_{ji})=1(x_{i}-w_{ji})=x_{i}-w_{ji}$ -> Les poids évoluent de la distance entre l’entrée courante et le neurone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas η ∈ ]0,1[, la valeur des poids du neurone **se rapprochera de l’entrée courante**. L’évolution sera plus ou moins conséquente selon que la valeur est plus proche de 1 ou 0.\n",
    "\n",
    "$w_{ij}=ηe^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{\\frac{-||0||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{0}(x_{i}-w_{ji})=η1(x_{i}-w_{ji})=η(x_{i}-w_{ji})$ -> 0<η<1, donc les poids évoluent de la distance entre l’entrée courante et le neurone multiplié par η et sera donc plus ou moins proche de l’entrée courante selon la valeur de η."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas η>1, la prochaine valeur des poids du neurone **dépassera celle de l’entrée courante**. Les poids évolueront plus vite que la distance qui les sépare de l’entrée courante.\n",
    "\n",
    "$w_{ij}=ηe^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{\\frac{-||0||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{0}(x_{i}-w_{ji})=η1(x_{i}-w_{ji})=η(x_{i}-w_{ji})$ -> η>1,\n",
    "donc les poids évoluent de la distance entre l’entrée courante et le neurone multiplié par η et dépasse la valeur de l’entrée courante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Influence de σ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas σ augmente, les neurones proches du neurone gagnant vont d'avantage apprendre de l’entrée courante\n",
    "\n",
    "Si σ augmente, $\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}$ tend vers 0, donc $e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}$ tend vers 1. Les poids des neurones proches du neurone gagnant vont d’avantage apprendre de l’entrée courante.\n",
    "\n",
    "Si σ diminue, $\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}$ tend vers l’infini, donc $e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}$ tend vers 0. Les poids des neurones proches du neurone gagnant vont moins apprendre de l’entrée courante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si σ est plus grand, a convergence, l’auto-organisation obtenue sera-t-elle plus resserrée, car un plus grand nombre de neurones seront influencés par chaque entrée courante, ce qui les rendra à therme plus semblable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons rechercher un indice de resserement de la carte auto-organisée. Pour cela, on va effectuer la moyenne des distances euclidiennes entre les poids des neurones et les poids des neurones voisins. Si cette moyenne est faible, cela signifie que les poids des neurones sont proches les uns des autres, et donc que la carte est resserrée.\n",
    "\n",
    "Pour tous j' voisin de j:\n",
    "    On calcule la distance entre j et j'\n",
    "\n",
    "On fait la somme des distances\n",
    "\n",
    "\n",
    "De manière plus mathématique:\n",
    "\n",
    "$coefficient\\_resserement = \\sum_{j} \\sum_{j'} ||w_{j}-w_{j'}||$\n",
    "pour j' voisin de j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Influence de la distribution d’entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas simple : 2 entrées $X_{1}$ et $X_{2}$\n",
    "Cas $X1$ et $X2$ sont présentés autant de fois, la valeur du poids du neurone $j$ convergera vers la valeur $\\frac{X_{1}+X_{2}}{2}$\n",
    "Cas $X1$ est présenté $n$ fois plus souvent que $X_{2}$, la valeur du poids du neurone $j$ convergera d'avantage vers la valeur de $X_{1}$ selon la valeur $n$\n",
    "Cas d’entrées provenant d’une base de données quelconque la valeur du poids du neurone $j$ convergera vers $X_{1}$ ou $X{2}$ selon la fréquence d’apparition de ces entrées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas d’une carte avec plusieurs neurones en se focalisant sur un neurone quelconque. \n",
    "\n",
    "Le neurone apprend :\n",
    "- Des entrées dont il est le plus proche\n",
    "- Des entrées dont un de ses voisins est le plus proche si $\\sigma$ est assez grand\n",
    "\n",
    "Le neurone apprend avec la force qui dépend de:\n",
    "- La distance entre l’entrée et le neurone\n",
    "- η\n",
    "- $\\sigma$ si l'entrée est plus proche d'un voisin que du neurone lui-même"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la répartition sur la carte est dense, les neuronnes seront concentré proche des entrées X, la mesure de quantification\n",
    "vectorielle sera faible\n",
    "\n",
    "Si la répartition sur la carte est peu dense, les neuronnes seront répartie de manière plus homogène dans l'espace afin mieu répondre au entrée, la mesure de quantification vectorielle sera élevé.\n",
    "\n",
    "En général, la densité des neurones sera similaire à la densité des Xi en entrée.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "![image.png](images/image1.png)  \n",
    "Au centre, la densité des entrées est plus élevée, les neurones sont concentrés autour de cette zone. La mesure de quantification vectorielle est faible.\n",
    "\n",
    "Autour, la densité des entrées est plus faible, les neurones sont répartis de manière plus homogène. La mesure de quantification vectorielle est élevée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Analyse de l'algorithme\n",
    "\n",
    "Nous voulons étudier l'impact des variables $\\eta$ et $\\sigma$ ainsi que la distribution des entrées dans l'apprentissage.\n",
    "\n",
    "### 1. Influence de $\\eta$\n",
    "\n",
    "Pour rappel, notre théorie était que $\\eta$ définissait la vitesse d'adaptation des neuronnes aux entrées.\n",
    "\n",
    "Nous supposons que si $\\eta$ est trop proche de 0, alors les neurones n'apprendront pas assez de chaque itération et que\n",
    "le modèle évoluera tout doucement.\n",
    "\n",
    "Nous supposons que si $\\eta$ est trop proche de 1, alors les neurones apprendront beaucoup à chaque itération et risque un \n",
    "surapprentissage.\n",
    "\n",
    "Nous supposons que si $\\eta$ est superieur à 1, alors les neurones apprendront trop à chaque itération et que le résultat\n",
    "ne convergera pas vers une bonne solution.\n",
    "\n",
    "Nous supposons que la valeur optimale se trouve dans l'intervalle $]0, 1[$, afin d'être plus précis, la valeur ne dois pas être trop grande.\n",
    "\n",
    "Afin d'analyser l'impact de $\\eta$ sur l'apprentissage, nous ferons varier cette valeur de $0.05$ à $2$.\n",
    "Nous fixons les autres paramètres à:\n",
    "\n",
    "$\\sigma$: 1.4\n",
    "\n",
    "$N$:1200\n",
    "\n",
    "Distribution: uniforme\n",
    "\n",
    "Nous noterons la qualité du reseau neuronnal selon l'erreur de quantification vectorielle moyenne.\n",
    "Nous observerons également l'évolution des neuronnes à chaques itérations.\n",
    "\n",
    "Les resultats que nous avons sont:\n",
    "\n",
    "![img.png](images/eta/values.png)\n",
    "\n",
    "La représentation graphique des résultat est:\n",
    "\n",
    "![img.png](images/eta/erreur_quantification_vectorielle.png)\n",
    "\n",
    "Les observations de l'évolution à chaque itération donne:\n",
    "\n",
    "**Pour $\\eta=0.005$**\n",
    "\n",
    "![](images/eta/eta_0.005.gif)\n",
    "\n",
    "**Pour $\\eta=0.01$**\n",
    "\n",
    "![](images/eta/eta_0.01.gif)\n",
    "\n",
    "**Pour $\\eta=0.1$**\n",
    "\n",
    "![](images/eta/eta_0.1.gif)\n",
    "\n",
    "**Pour $\\eta=0.3$**\n",
    "\n",
    "![](images/eta/eta_0.3.gif)\n",
    "\n",
    "**Pour $\\eta=0.5$**\n",
    "\n",
    "![](images/eta/eta_0.5.gif)\n",
    "\n",
    "**Pour $\\eta=1$**\n",
    "\n",
    "![](images/eta/eta_1.gif)\n",
    "\n",
    "**Pour $\\eta=2$**\n",
    "\n",
    "![](images/eta/eta_2.gif)\n",
    "\n",
    "On en conclue que:\n",
    "\n",
    "Un $\\eta$ trop petit a bien pour conséquence une évolution trop petite des neurones de chaque itération. Pour $\\eta=0.005$ \n",
    "et $\\eta=0.01$, la carte de neurone n'a pas eu le temps de se \"déplier\" avant la fin de l'entrenement et le résultat est donc mauvais.\n",
    "\n",
    "Un $\\eta$ trop grand a bien pour conséquence un surapprentissage qui ne converge pas vers une bonne solution.\n",
    "$\\eta=1$ et $\\eta=2$ sont beaucoup trop grand et ne converge vers rien\n",
    "\n",
    "Un $\\eta$ entre 0 et 1, donne bien des meilleur valeurs. De même, comme supposé, une valeur plutt faible donne une réponse plus\n",
    "optimal. Ainsi la meilleur solution est $\\eta=0.1$ suivie de $\\eta=0.3$ suivie de $\\eta=0.5$.\n",
    "\n",
    "Notre théorie était donc correct.\n",
    "\n",
    "#### Cas où $\\eta$ est proche de 0\n",
    "\n",
    "Dans la partie [3], nous avions supposé que si $\\eta$ était proche de 0, alors l'apprentissage serait lent. \n",
    "Il y aurait peu de variation par itération.\n",
    "\n",
    "\n",
    "Durant cette expérimentation, nous avons fixé $\\eta$ à $0.01$. On remarque que le poids des neurones varient très \n",
    "lentement par rapport à un $\\eta$ qui serait plus grand.\n",
    "\n",
    "\n",
    "![etat_0.01](images/eta/eta_0.01.gif)\n",
    "\n",
    "Erreur de quantification vectorielle\n",
    "\n",
    "0.01:0.010142765151257655 1 * 10^-2\n",
    "\n",
    "#### Cas où $\\eta$ est 0.1\n",
    "\n",
    "\n",
    "Erreur de quantification vectorielle :\n",
    "E = 0.009079728797496696 = 9 * 10^-3\n",
    "\n",
    "![etat_0.1](images/eta/eta_0.1.gif)\n",
    "\n",
    "#### Cas où $\\eta$ est 0.3\n",
    "\n",
    "// TODO\n",
    "\n",
    "Erreur de quantification vectorielle :\n",
    "E = 0.0077876184755363625 = 7 * 10^-3\n",
    "\n",
    "![etat_0.3](images/eta/eta_0.3.gif)\n",
    "\n",
    "#### Cas où $\\eta$ est 0.5\n",
    "\n",
    "// TODO\n",
    "\n",
    "Erreur de quantification vectorielle :\n",
    "E = 0.00812546936589448 = 8 * 10-3\n",
    "\n",
    "![etat_0.5](images/eta/eta_0.5.gif)\n",
    "\n",
    "\n",
    "### 2. Influence de $\\sigma$\n",
    "\n",
    "Afin d'étudier l'influence de $\\sigma$ sur la variation de la valeur vecteur poids $W_{ji}$ associé à un neuronne ${j} nous fixons les paramètres suivants:\n",
    "- $\\eta = 1,4$\n",
    "- $N = 3000$ \n",
    "\n",
    "### Cas où $\\sigma$ est proche de 0\n",
    "\n",
    "On remarque que quand sigma est proche de 0, l'auto-organisation de la carte est beaucoup plus lâche. Cela est dû au fait\n",
    "que $\\exp^{-X}$, tel que $X = \\frac{|| j - j^{*}||^{2}}{2 * \\sigma^{2} }$, tend vers $1$. Donc, chaque neurone va se rapprocher de son entrée Xi \n",
    "associé. Cela cause une dispersion des neurones sur la carte, d'où le fait qu'on a une valeur de dispersion des voisins\n",
    "très élevé: $dispersion = 1,74 * 10^{2}$\n",
    "\n",
    "Voici l'apprentissage du réseau de neurones avec un $\\sigma=0.01$\n",
    "\n",
    "![sigma_0.1](images/sigma/sigma_0.01.gif)\n",
    "\n",
    "### Cas où $\\sigma$ augmente\n",
    "\n",
    "Quand $_sigma$ augmente, $\\exp^{-X}$, tel que $X = \\frac{|| j - j^{*}||^{2}}{2 * \\sigma^{2} }$, tend vers 0. La variation du poids associé à un neuronne\n",
    "va alors stagner ce qui cause un regroupement de tous les neuronnes dans un espace restreint sur la carte.\n",
    "\n",
    "On le remarque encore plus avec la valeur de dispersion pour des valeurs suffisament hautes de sigma:\n",
    "\n",
    "$\\sigma = 5$ => $\\_coeff\\_de_dispersion$ = 1.59$\n",
    "$\\sigma = 20$ => $\\_coeff\\_de_dispersion$ = 1*10^-2$\n",
    "\n",
    "Pour $\\sigma \\in {5, 20}$\n",
    "\n",
    "$\\sigma = 5$\n",
    "\n",
    "![sigma_5](images/sigma/sigma_5.gif)\n",
    "\n",
    "$\\sigma = 20$\n",
    "\n",
    "![sigma_20](images/sigma/sigma_20.gif)\n",
    "\n",
    "### Evolution de la distance totale \n",
    "\n",
    "![distance_voisins](images/sigma/distance_voisins.png)\n",
    "\n",
    "On remarque bien que plus $\\sigma$ est grand, plus la distance entre les voisins est grande.\n",
    "En revanche, plus $\\sigma$ est petit, plus la variation du vecteur poids $W_{j}$ associé au neurone ${j} sera faible. Les neurones du réseau ne se dirigent donc pas vers une entrée $X$, donc leurs \n",
    "voisins non plus. Cela implique que tous les $W_{j}$ seront plus ou moins égaux à chaque pas de temps\n",
    "\n",
    "### 3. Nombre de pas de temps d'apprentissage $N$\n",
    "\n",
    "### 4. Influence de la distribution d’entrée\n",
    "\n",
    "On observe que les premières entrées ont un rôle trè important dans le résultat.\n",
    "Si les premières entrées permettent de \"déplier\" le maillage de neurrone, \n",
    "\n",
    "### 5. Topologie de la carte (fonction de voisinage, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nombre de pas de temps d'apprentissage $N$\n",
    "\n",
    "### 4. Influence de la distribution d’entrée\n",
    "\n",
    "On observe que les premières entrées ont un rôle trè important dans le résultat.\n",
    "Si les premières entrées permettent de \"déplier\" le maillage de neurrone, \n",
    "\n",
    "### 5. Topologie de la carte (fonction de voisinage, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
