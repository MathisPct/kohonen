{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Etude théorique de cas simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Influence de η\n",
    "\n",
    "**Cas η = 0**: La valeur des poids du neurone **ne changeront pas**. En effet η est le taux d’apprentissage, il permet de configurer la vitesse à laquelle les neurones doivent apprendre à chaque itération. S’il vaut 0, les neurones n’évolueront pas entre deux itérations. \n",
    "\n",
    "$w_{ij}=0e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=0$ -> Aucune évolutions des poids\n",
    "\n",
    "<br>\n",
    "\n",
    "**Cas η=1**: La valeur des poids du neurone **seront celle de l’entrée courante**. Les neurones auront appris de l’itération et les poids auront évoluer. Comme $η=1$, ils auront compensé toute la distance qui sépare notre neurone à l’entrée courante.\n",
    "\n",
    "$w_{ij}=1e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=e^{\\frac{-||0||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=e^{0}(x_{i}-w_{ji})=1(x_{i}-w_{ji})=x_{i}-w_{ji}$ -> Les poids évoluent de la distance entre l’entrée courante et le neurone\n",
    "\n",
    "<br>\n",
    "\n",
    "**Cas η ∈ ]0,1[**: La valeur des poids du neurone **se rapprochera de l’entrée courante**. L’évolution sera plus ou moins conséquente selon que la valeur est plus proche de 1 ou 0.\n",
    "\n",
    "$w_{ij}=ηe^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{\\frac{-||0||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{0}(x_{i}-w_{ji})=η1(x_{i}-w_{ji})=η(x_{i}-w_{ji})$ -> 0<η<1, donc les poids évoluent de la distance entre l’entrée courante et le neurone multiplié par η et sera donc plus ou moins proche de l’entrée courante selon la valeur de η.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Cas η>1**: La prochaine valeur des poids du neurone **dépassera celle de l’entrée courante**. Les poids évolueront plus vite que la distance qui les sépare de l’entrée courante.\n",
    "\n",
    "$w_{ij}=ηe^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{\\frac{-||0||^{2}_{c}}{2\\sigma^{2}}}(x_{i}-w_{ji})=ηe^{0}(x_{i}-w_{ji})=η1(x_{i}-w_{ji})=η(x_{i}-w_{ji})$ -> η>1,\n",
    "donc les poids évoluent de la distance entre l’entrée courante et le neurone multiplié par η et dépasse la valeur de l’entrée courante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Influence de σ\n",
    "\n",
    "**Cas σ augmente**, les neurones proches du neurone gagnant vont **apprendre d'avantage** de l’entrée courante\n",
    "\n",
    "En effet, si $σ$ augmente, $\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}$ tend vers 0, donc $e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}$ tend vers 1. Les poids des neurones proches du neurone gagnant vont d’avantage apprendre de l’entrée courante.\n",
    "\n",
    "Par conséquent, si $σ$ diminue, $\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}$ tend vers l’infini, donc $e^{\\frac{-||j-j^{*}||^{2}_{c}}{2\\sigma^{2}}}$ tend vers 0. Les poids des neurones proches du neurone gagnant vont moins apprendre de l’entrée courante.\n",
    "\n",
    "<br>\n",
    "\n",
    "Si σ est plus grand, a convergence, l’auto-organisation obtenue **sera plus resserrée**, car un plus grand nombre de neurones seront influencés par chaque entrée courante, ce qui les rendra à therme plus semblable.\n",
    "\n",
    "<br>\n",
    "\n",
    "Nous allons rechercher un indice de resserement de la carte auto-organisée. Pour cela, nous effectuerons la somme des distances euclidiennes entre les poids des neurones et les poids des neurones voisins. Si cette somme est faible, cela signifie que les poids des neurones sont proches les uns des autres, et donc que la carte est resserrée.\n",
    "\n",
    "Pseudo-code:\n",
    "```pseudo\n",
    "Pour tous j' voisin de j:\n",
    "   On calcule la distance entre j et j'\n",
    "\n",
    "On fait la somme des distances\n",
    "```\n",
    "<br>\n",
    "\n",
    "De manière plus mathématique:\n",
    "\n",
    "$coefficient\\_resserement = \\sum_{j} \\sum_{j'} ||w_{j}-w_{j'}||$\n",
    "pour j' voisin de j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Influence de la distribution d’entrée\n",
    "\n",
    "**Cas simple, 2 entrées $X_{1}$ et $X_{2}$**\n",
    "\n",
    "Cas $X1$ et $X2$ sont présentés autant de fois, la valeur du poids du neurone $j$ convergera vers la valeur $\\frac{X_{1}+X_{2}}{2}$\n",
    "\n",
    "Cas $X1$ est présenté $n$ fois plus souvent que $X_{2}$, la valeur du poids du neurone $j$ convergera d'avantage vers la valeur de $X_{1}$ selon la valeur $n$\n",
    "\n",
    "Cas d’entrées provenant d’une base de données quelconque la valeur du poids du neurone $j$ convergera vers $X_{1}$ ou $X{2}$ selon la fréquence d’apparition de ces entrées.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Dans le cas d’une carte avec plusieurs neurones en se focalisant sur un neurone quelconque.** \n",
    "\n",
    "Le neurone apprend :\n",
    "- Des entrées dont il est le plus proche\n",
    "- Des entrées dont un de ses voisins est le plus proche si $\\sigma$ est assez grand\n",
    "\n",
    "Le neurone apprend avec la force qui dépend de:\n",
    "- La distance entre l’entrée et le neurone\n",
    "- η\n",
    "- $\\sigma$ si l'entrée est plus proche d'un voisin que du neurone lui-même\n",
    "\n",
    "<br>\n",
    "\n",
    "Si la répartition sur la carte est dense, les neuronnes seront concentré proche des entrées X, la mesure de quantification\n",
    "vectorielle sera faible\n",
    "\n",
    "Si la répartition sur la carte est peu dense, les neuronnes seront répartie de manière plus homogène dans l'espace afin mieu répondre au entrée, la mesure de quantification vectorielle sera élevé.\n",
    "\n",
    "En général, la densité des neurones sera similaire à la densité des $X_{i}$ en entrée.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "![image.png](images/image1.png)  \n",
    "Au centre, la densité des entrées est plus élevée, les neurones sont concentrés autour de cette zone. La mesure de quantification vectorielle est faible.\n",
    "\n",
    "Autour, la densité des entrées est plus faible, les neurones sont répartis de manière plus homogène. La mesure de quantification vectorielle est élevée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2: Analyse de l'algorithme\n",
    "\n",
    "Nous voulons étudier l'impact des variables $\\eta$, $\\sigma$, $N$ ainsi que la distribution des entrées dans l'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Influence de $\\eta$\n",
    "\n",
    "Pour rappel, notre théorie était que $\\eta$ définissait la vitesse d'adaptation des neuronnes aux entrées.\n",
    "\n",
    "Nous supposons que si $\\eta$ est trop proche de 0, alors les neurones n'apprendront pas assez de chaque itération et que le modèle évoluera trop doucement.\n",
    "\n",
    "Nous supposons que si $\\eta$ est trop proche de 1, alors les neurones apprendront beaucoup de chaque itération et risque un surapprentissage.\n",
    "\n",
    "Nous supposons que si $\\eta$ est superieur à 1, alors les neurones apprendront trop à chaque itération et que le résultat ne convergera pas vers une bonne solution.\n",
    "\n",
    "Nous supposons que la valeur optimale se trouve dans l'intervalle $]0, 1[$, ni trop proche de 0, ni de 1. Afin de convergé vers une position optimal, $\\eta$ dois être plutot petit.\n",
    "\n",
    "<br>\n",
    "\n",
    "Afin d'analyser l'impact de $\\eta$ sur l'apprentissage, nous ferons varier cette valeur de $0.005$ à $2$.\n",
    "Nous fixons les autres paramètres à:\n",
    "\n",
    "- $\\sigma: 1.4$\n",
    "- $N: 30000$\n",
    "- $Distribution: [−1, 1] × [−1, 1]$\n",
    "\n",
    "<br>\n",
    "\n",
    "Nous noterons la qualité du reseau neuronnal selon l'erreur de quantification vectorielle moyenne.\n",
    "Nous observerons également l'évolution des neuronnes à chaques itérations.\n",
    "\n",
    "Les resultats que nous avons obtenu en faisant varier $\\eta$ sont les suivants :\n",
    "\n",
    "![img.png](images/eta/values.png)\n",
    "\n",
    "Ainsi que la représentation graphique :\n",
    "\n",
    "![img.png](images/eta/erreur_quantification_vectorielle.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Les observations de l'évolution de $\\eta$ entre $0.005$ à $2$ à chaque itération donne:\n",
    "\n",
    "**Pour $\\eta=0.005$**\n",
    "\n",
    "![](images/eta/eta_0.005.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pour $\\eta=0.01$**\n",
    "\n",
    "![](images/eta/eta_0.01.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pour $\\eta=0.1$**\n",
    "\n",
    "![](images/eta/eta_0.1.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pour $\\eta=0.3$**\n",
    "\n",
    "![](images/eta/eta_0.3.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pour $\\eta=0.5$**\n",
    "\n",
    "![](images/eta/eta_0.5.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pour $\\eta=1$**\n",
    "\n",
    "![](images/eta/eta_1.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pour $\\eta=2$**\n",
    "\n",
    "![](images/eta/eta_2.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "**On en conclue que:**\n",
    "\n",
    "Pour $\\eta=0.005$ et $\\eta=0.01$, par l'observation visuelle, on remaque que la carte de neurone n'a pas eu le temps de se \"déplier\" avant la fin de l'entraînement.\n",
    "Cette observation est confirmée par les résultats, en effet pour $\\eta=0.005$, le valeur de l'erreur de quantification vectorielle moyenne est 0,022793208 et pour $\\eta=0.01$ elle est de 0,02325148.\n",
    "Un $\\eta$ trop petit à donc bien pour conséquence une évolution trop faible des neurones pour chaque itération. Le réseau de neurones n'a pas le temps de s'adapter aux données lors de l'entraînement.\n",
    "\n",
    "Pour $\\eta=1$ et $\\eta=2$, par l'observation visuelle, on remarque que les neurones ne converge jamais vers une position stable.\n",
    "Cette observation est confirmée par les résultats, en effet pour $\\eta=1$, le valeur de l'erreur de quantification vectorielle moyenne est 0,027899027 et pour $\\eta=2$ elle est de 0,032545542.\n",
    "Un $\\eta$ trop grand à bien pour conséquence un surapprentissage qui ne converge pas vers une bonne solution et vers la distribution des données qu'on reçoit en entrée.\n",
    "\n",
    "Pour $\\eta=0.1$, $\\eta=0.3$ et $\\eta=0.5$, par l'observation visuelle, on remarque que les neurones converge vers une position stable.\n",
    "Cette observation est confirmé par les résultats, en effet pour $\\eta=0.1$, le valeur de l'erreur de quantification vectorielle moyenne est 0,013436864, pour $\\eta=0.3$ elle est de 0,017003551 et pour $\\eta=0.5$ de 0,018124309.\n",
    "Un $\\eta$ entre 0 et 1, donne bien de meilleurs résultat. Les neurones ont le temps de se répartir pour s'adapter au mieu aux vecteurs en entrée et converge vers une position stable.\n",
    "\n",
    "Notre théorie était donc correct. On remarque que dans notre cas le $\\eta$ optimal se trouve entre $0.01$ et $0.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Influence de $\\sigma$\n",
    "\n",
    "Afin d'étudier l'influence de $\\sigma$ sur la variation de la valeur du vecteur poids $W_{ji}$ associé à un neurone ${j}$, nous fixons les paramètres suivants :\n",
    "\n",
    "- $\\eta = 1,4$\n",
    "- $N = 30000$\n",
    "- $\\text{Distribution}: [-1, 1] \\times [-1, 1]$\n",
    "\n",
    "### Cas où $\\sigma$ est proche de 0\n",
    "\n",
    "On remarque que quand sigma est proche de 0, l'auto-organisation de la carte est beaucoup plus lâche. Cela est dû au fait\n",
    "que $\\exp^{-X}$, tel que $X = \\frac{|| j - j^{*}||^{2}}{2 * \\sigma^{2} }$, tend vers $1$. Donc, chaque neurone va se rapprocher de son entrée Xi \n",
    "associé. Cela cause une dispersion des neurones sur la carte, d'où le fait qu'on a une valeur de dispersion des voisins\n",
    "très élevé: $dispersion = 1,74 * 10^{2}$\n",
    "\n",
    "Voici l'apprentissage du réseau de neurones avec un $\\sigma=0.01$\n",
    "\n",
    "![sigma_0.1](images/sigma/sigma_0.01.gif)\n",
    "\n",
    "### Cas où $\\sigma$ augmente\n",
    "\n",
    "Lorsque $\\sigma$ augmente, $\\exp^{-X}$, avec $X = \\frac{| j - j^{*} |^{2}}{2\\sigma^{2}}$, tend vers 0. La variation du poids associé à un neurone va alors stagner, ce qui cause un regroupement de tous les neurones dans un espace restreint sur la carte.\n",
    "\n",
    "On le remarque encore plus avec la valeur de dispersion qui diminue avec l'augmentation de $\\sigma$ :\n",
    "\n",
    "$\\sigma = 5 \\Rightarrow \\text{dispersion} = 1.59$\n",
    "$\\sigma = 20 \\Rightarrow \\text{dispersion} = 1 \\times 10^{-2}$\n",
    "\n",
    "Voici la variation des apprentissages du réseau de neurones pour $\\sigma \\in {5, 20}$ :\n",
    "\n",
    "**$\\sigma = 5$**\n",
    "\n",
    "![sigma_5](images/sigma/sigma_5.gif)\n",
    "\n",
    "**$\\sigma = 20$**\n",
    "\n",
    "![sigma_20](images/sigma/sigma_20.gif)\n",
    "\n",
    "### Evolution de la distance totale \n",
    "\n",
    "![distance_voisins](images/sigma/distance_voisins.png)\n",
    "\n",
    "On remarque bien que plus la valeur de $\\sigma$ est petite, plus la distance entre les voisins est grande. En revanche, plus $\\sigma$ est grand, plus la variation du vecteur poids $W_j$ associé au neurone ${j}$ sera faible. Les neurones du réseau ne se dirigent donc pas vers une entrée $X$, donc leurs voisins non plus. Cela implique que tous les $W_j$ seront plus ou moins égaux à chaque pas de temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nombre de pas de temps d'apprentissage $N$\n",
    "\n",
    "Afin d'étudier l'influence de $N$ nous fixons les paramètres suivants:\n",
    "\n",
    "- $\\eta: 0.05$\n",
    "- $\\sigma: 1.4$\n",
    "- $Distribution: [−1, 1] × [−1, 1]$\n",
    "\n",
    "<br>\n",
    "\n",
    "Les resultats que nous avons sont:\n",
    "\n",
    "![Alt text](./images/N/values.png)\n",
    "\n",
    "La représentation graphique des résultat est:\n",
    "\n",
    "![Alt text](./images/N/graph1.png)\n",
    "\n",
    "![alt text](./images/N/graph2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "On remarque par les résultats que plus notre réseau de neurones à de cycles d'apprentissages, plus il devient efficace (erreur de quantification vectorielle moyenne faible et coef de resserement faible).\n",
    "\n",
    "Toutefois la progression est logaritmique, l'amélioration des résultat est de plus en plus faible par rapport à l'augmentation du nombre de cycle d'apprentissage.\n",
    "\n",
    "Il est donc important de prendre une valeur de $N$ importante afin d'affiner le résultat mais celle-ci ne dois pas être trop grande car elle augmente le temps d'entrainement de façon significative pour une amélioration de moins en moins perceptible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. Influence de la distribution d’entrée\n",
    "\n",
    "Afin d'étudier l'influence de la distribution des entrée nous fixons les paramètres suivants:\n",
    "\n",
    "- $\\eta: 0.05$\n",
    "- $\\sigma: 1.4$\n",
    "- $N: 30000$\n",
    "\n",
    "<br>\n",
    "\n",
    "Les resultats que nous avons sont:\n",
    "\n",
    "![Alt text](./images/config/values.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "$Distribution: [−1, 1] × [-1, 1]$\n",
    "\n",
    "![Alt text](./images/config/config1.png)\n",
    "\n",
    "![Alt text](./images/config/config1.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "$Distribution: [−1, 0] × [0, 1], [−1, 0] × [−1, 0] et [0, 1] × [−1, 0]$\n",
    "\n",
    "![Alt text](./images/config/config2.png)\n",
    "\n",
    "![Alt text](./images/config/config2.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "$Distribution: [−1, 0] × [0, 1] et [0, 1] × [−1, 0]$\n",
    "\n",
    "![Alt text](./images/config/config3.png)\n",
    "\n",
    "![Alt text](./images/config/config3.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "$Distribution: [−1, 0] × [-1, 1] et 3*[0, 1] × [-1, 1] $\n",
    "\n",
    "![Alt text](./images/config/config4.png)\n",
    "\n",
    "![Alt text](./images/config/config4.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "$Distribution: [−1, 0] × [-1, 1] et 7*[0, 1] × [-1, 1] $\n",
    "\n",
    "![Alt text](./images/config/config5.png)\n",
    "\n",
    "![Alt text](./images/config/config5.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "$Distribution: [−1, 0] × [-1, 1] et 3*[0, 1] × [0, 1] $\n",
    "\n",
    "![Alt text](./images/config/config6.png)\n",
    "\n",
    "![Alt text](./images/config/config6.gif)\n",
    "\n",
    "<br>\n",
    "\n",
    "On observe que les réseaux de neurone s'adaptent à la distribution respectives de leurs entrées.\n",
    "\n",
    "Lors de nos différents testes (qui ne sont pas tous répertorié ici), on a pu observer que les premières entrées avaient un rôle très important dans le résultat.\n",
    "\n",
    "Si les premières entrées permettent de \"déplier\" le maillage de neurrone, le résultat final est meilleur\n",
    "\n",
    "### 5. Topologie de la carte (fonction de voisinage, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 : Bras robotique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour prédire la position qu’aura la main étant donné une position du bras ($\\Theta_{1}$,$\\Theta_{2}$):\n",
    "1. On entraine notre réseau neuronnale sur les données mise à notre disposition.\n",
    "2. On regarde dans notre réseau de neuronne le neurone qui a la position la plus proche de notre bras.\n",
    "3. On renvoie la position de la main associée à ce neurone.\n",
    "\n",
    "Cela repose sur la logique:\n",
    "\n",
    "Si je sais que pour la position de mon bras ($\\Theta_{1}$,$\\Theta_{2}$), ma main est à la position $(x_{1},x_{2})$, et que je recherche la postion pour $\\Theta_{1}+a$ et $\\Theta_{2}+b$ sachant que $a$ et $b$ sont faible, je peux en conclure que ma main sera **proche de** $(x_{1},x_{2})$\n",
    "\n",
    "<br>\n",
    "\n",
    "Solution d'implémentation:\n",
    "\n",
    "V1: La façon la plus triviale de le mettre en place est de rechercher dans ma carte l'entrée la plus proche, puis de renvoyer la valeur de son bras\n",
    "\n",
    "V2: Une version améliorée serait de prendre les $x$ neuronnes les plus proches et de faire la moyenne des valeurs de leurs bras\n",
    "\n",
    "V3: Enfin la version la plus aboutis serait de prendre les $x$ neuronnes les plus proches et de faire la moyenne pondéré selon la distance à l'entrée des valeurs de leurs bras.\n",
    "\n",
    "##### Légende des graphiques:\n",
    "\n",
    "Graphique de gauche:\n",
    "- En vert, la position du bras donné pour laquelle il faut prédir la position de la main\n",
    "- En rouge, la position du ou des neurones les plus proches de la position du bras donné utilisé pour la prédiction\n",
    "\n",
    "Graphique de droite:\n",
    "- En vert, la position de la main exacte pour la position du bras donné (calculé via la formule) afin de comparer avec la prédiction\n",
    "- En rouge, la position de la main prédite\n",
    "\n",
    "Sur le dernier graphique, on compare les résultats des différentes solutions\n",
    "- En vert, la position de la main exacte pour la position du bras donné (calculé via la formule) afin de comparer avec la prédiction\n",
    "- En rouge, la position de la main prédite avec la solution V1\n",
    "- En bleu, la position de la main prédite avec la solution V2 (caché par le jaune sur le graphique)\n",
    "- En jaune, la position de la main prédite avec la solution V3\n",
    "\n",
    "Résultats obtenus pour l'entrainement de notre réseau de neuronne:\n",
    "\n",
    "![](./images/bras/bras.gif)\n",
    "\n",
    "Résultats obtenus avec la solution V1:\n",
    "\n",
    "![](./images/bras/figure1.png)\n",
    "\n",
    "Résultats obtenus avec la solution V2:\n",
    "\n",
    "![](./images/bras/figure2.png)\n",
    "\n",
    "Résultats obtenus avec la solution V3:\n",
    "\n",
    "![](./images/bras/figure3.png)\n",
    "\n",
    "Comparaison des résultats des différentes solutions:\n",
    "\n",
    "![](./images/bras/figure4.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Afin de comparer les différentes solutions, nous avons calculé la distance euclidienne entre la position de la main exacte et la position de la main prédite pour 10 valeurs aléatoire de bras en réentrainant le réseau de neuronne à chaque fois. On obtient les résultats suivants:\n",
    "\n",
    "![alt text](./images/bras/values.png)\n",
    "\n",
    "![alt text](./images/bras/graph.png)\n",
    "\n",
    "On remarque que la solution est toujours aussi ou plus précise que la solution 2, toutefois elle n'est pas toujours plus précise que la solution 1. Cela est dû au fait que la solution 1 est plus précise pour les valeurs de bras proche de celles entrainé, alors que la solution 3 est plus précise pour les valeurs de bras plus éloigné de celles entrainé. Autrement dit, si une valeur de bras est très proche de celles d'un neuronne entrainé, la solution 1 sera plus précise, sinon la solution 3 sera plus précise. L'algorithme de la solution 1 est sans doute plus adapté quand le maillage de neuronne est plus fin, alors que la solution 3 est plus adapté quand le maillage de neuronne est plus grossier. De plus, la solution 3 est globalement plus stable que la solution 1.\n",
    "\n",
    "Il est également important de noter que la présision des solutions dépend de la qualité de l'entrainement du réseau de neuronne. Plus le réseau de neuronne est entrainé, plus les solutions seront précises. Dans certain test (le $1^{er}$, $4^{ème}$ et $5^{ème}$), le réseau de neuronne s'est mal \"déplié\" et les solutions étaient moins précises.\n",
    "\n",
    "<br>\n",
    "\n",
    "Pour prédire la position du bras étant donné une position de la main, il suffit de faire l'inverse. On regarde dans notre carte le neuronne pour la position la plus proche de notre main et l'on renvoie la position du bras associé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour prédire la suite des positions spatiales prise par la main lors du déplacement du bras d'une position motrice $(\\Theta_{1},\\Theta_{2})$ à $(\\Theta'_{1},\\Theta'_{2})$ on divise tout d'abord le trajet en $x$ point de passage. Si $x$ est grand, alors le tracé sera plus précis. Si $x$ est petit, alors le tracé sera plus vague.\n",
    "\n",
    "Par la suite on fait une estimation de la postion de la main pour chaque point de passage. On obtient alors une suite de position de la main qui représente le tracé de la main lors du déplacement du bras.\n",
    "\n",
    "Résultats obtenus avec la solution V1:\n",
    "\n",
    "![](./images/bras/figure5.png)\n",
    "\n",
    "Résultats obtenus avec la solution V2:\n",
    "\n",
    "![](./images/bras/figure6.png)\n",
    "\n",
    "Résultats obtenus avec la solution V3:\n",
    "\n",
    "![](./images/bras/figure7.png)\n",
    "\n",
    "Comparaison des résultats des différentes solutions:\n",
    "\n",
    "![](./images/bras/figure8.png)\n",
    "\n",
    "Encore une fois le tracé bleu est grandement caché par le jaune sur le graphique de droite. Ceci est dû au fait que le version 2 et 3 sont très proche l'une de l'autre.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
